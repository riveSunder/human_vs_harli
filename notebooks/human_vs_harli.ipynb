{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553f16d9",
   "metadata": {},
   "source": [
    "# Resetting the Wave:\n",
    "## A High Reward Strategy Employed by a Hebbian Cellular Automaton Policy to Game a Change-in-Center-of-Mass Mobility Reward Across Multiple B3/Sxxx Life-Like Rules.  \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../assets/harli_reset_wave_strategy_small.gif\">\n",
    "    <em>\n",
    "        First observation of the \"reset wave\" strategy for gaming the `SpeedDetector` reward wrapper in Carle's Game. Unlike the trained policies demonstrated in this notebook, the animation above is of an agent operating in and trained on the B368/S245 Morley/Move rules.\n",
    "    </em>\n",
    "</div> \n",
    "   \n",
    "\n",
    "The command for running this experiment is shown below. The training run with the most successful and interesting take on gaming the reward function occured with random seed `42`. Coincidence? Probably. \n",
    "\n",
    "```\n",
    "experiment.py -mg  128  -ms  256  -p  32  -sm  1  -v  1  -d  cuda:1  -dim  128  -s  13  42  1337  -a  HARLI  -w  RND2D  SpeedDetector  -tr  B3/S023  B3/S236  B3/S237  B3/S238  -vr  B3/S23  -tag  _harli_glider_experiment_ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c91b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n",
    "\n",
    "import os \n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from carle.env import CARLE\n",
    "from carle.mcl import CornerBonus, SpeedDetector, PufferDetector, AE2D, RND2D\n",
    "from game_of_carle.agents.harli import HARLI\n",
    "from game_of_carle.agents.carla import CARLA\n",
    "from game_of_carle.agents.grnn import ConvGRNN\n",
    "from game_of_carle.agents.toggle import Toggle\n",
    "\n",
    "import bokeh\n",
    "import bokeh.io as bio\n",
    "from bokeh.io import output_notebook, show, curdoc\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import TextInput, Button, Paragraph\n",
    "from bokeh.models import ColumnDataSource\n",
    "\n",
    "from bokeh.events import DoubleTap, Tap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "my_cmap = plt.get_cmap(\"magma\")\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_string = \"cpu\"\n",
    "\n",
    "obs_dim = 32\n",
    "act_dim = 6\n",
    "\n",
    "agent =  HARLI(device=device_string, obs_dim=obs_dim, act_dim=act_dim)\n",
    "\n",
    "policy_list = []\n",
    "\n",
    "directory_list = os.listdir(\"../policies/\")\n",
    "\n",
    "for filename in directory_list:\n",
    "    \n",
    "    if \"HARLI\" in filename and \"glider\" in filename:\n",
    "        \n",
    "        policy_list.append(os.path.join(\"..\", \"policies\", filename))\n",
    "        \n",
    "policy_list.sort()\n",
    "# instantiate CARLE with a speed detection wrapper\n",
    "env = CARLE(height=obs_dim, width=obs_dim, action_height=act_dim, action_width=act_dim, device=device_string)\n",
    "env = SpeedDetector(env)\n",
    "\n",
    "# set rules\n",
    "# this agent was trained on B3/S023, B3/S236, B3/S237, and B3/S238\n",
    "my_rules = \"B3678/S34678\"\n",
    "\n",
    "agent.set_params(np.load(policy_list[0]))\n",
    "print(f\"{len(policy_list)} HARLI mobility policies found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc04792",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_index, len(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82c549",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "rules = [\"B3/S23\", \\\n",
    "    \"B368/S245\", \\\n",
    "    \"B3678/S34678\"]\n",
    "\n",
    "\n",
    "def modify_doc(doc):\n",
    "      \n",
    "    def update():\n",
    "        global obs\n",
    "        global stretch_pixel\n",
    "        global action\n",
    "        global agent_on\n",
    "        global agent_done\n",
    "        global my_step\n",
    "        global max_step\n",
    "        global rule_index\n",
    "        global rewards\n",
    "        global reward_sum\n",
    "               \n",
    "            \n",
    "        if rule_index == len(rules) and my_step >= max_steps:\n",
    "            \n",
    "            agent_off()\n",
    "            \n",
    "        elif my_step >= max_steps:\n",
    "            reset_next_ruleset()\n",
    "        else:\n",
    "\n",
    "            obs, r, d, i = env.step(action)\n",
    "            rewards = np.append(rewards, r.cpu().numpy().item())\n",
    "            if agent_on:\n",
    "                action = agent(obs) \n",
    "            else:\n",
    "                action = torch.zeros_like(action)\n",
    "\n",
    "            padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "\n",
    "            my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "            my_img[my_img > 3.0] = 3.0\n",
    "            (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "\n",
    "            new_data = dict(my_image=[my_img])\n",
    "\n",
    "            my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "            new_weights = dict(my_image=[my_weights])\n",
    "\n",
    "            #new_line = dict(x=np.arange(my_step+2), y=rewards)\n",
    "            new_line = dict(x=[my_step], y=[r.cpu().numpy().item()])\n",
    "\n",
    "            source.stream(new_data, rollover=1)\n",
    "            source_plot.stream(new_line, rollover=2000)\n",
    "\n",
    "            source_weights.stream(new_weights, rollover=1)\n",
    "\n",
    "            my_step += 1\n",
    "            reward_sum += r.item()\n",
    "            \n",
    "            message.text = f\"step {my_step}, reward: {r.item():.4f}, mean reward per step: {(reward_sum/my_step):.4f} \\n\"\\\n",
    "                    f\"{policy_list[0]} rule index = {rule_index}\"\n",
    "        \n",
    "        \n",
    "    def go():\n",
    "       \n",
    "        if button_go.label == \"Run >\":\n",
    "            my_callback = doc.add_periodic_callback(update, my_period)\n",
    "            button_go.label = \"Pause\"\n",
    "            #doc.remove_periodic_callback(my_callback)\n",
    "            \n",
    "        else:\n",
    "            doc.remove_periodic_callback(doc.session_callbacks[0])\n",
    "            button_go.label = \"Run >\"\n",
    "            \n",
    "        doc.remove_root(button_start)\n",
    "        \n",
    "        doc.remove_root(message1)\n",
    "        doc.remove_root(message2)\n",
    "        doc.remove_root(message3)\n",
    "        doc.remove_root(message4)\n",
    "        doc.remove_root(message5)\n",
    "    \n",
    "    def faster():\n",
    "        \n",
    "        \n",
    "        global my_period\n",
    "        my_period = max([my_period * 0.5, 32])\n",
    "        go()\n",
    "        go()\n",
    "        \n",
    "    def slower():\n",
    "        \n",
    "        global my_period\n",
    "        my_period = min([my_period * 2, 8192])\n",
    "        go()\n",
    "        go()\n",
    "        \n",
    "        \n",
    "    def reset_next_ruleset():\n",
    "       \n",
    "        global obs\n",
    "        global action\n",
    "        global stretch_pixel\n",
    "        global my_step\n",
    "        global rewards\n",
    "        global use_spaceship\n",
    "        global rewards\n",
    "        global reward_sum\n",
    "        global rule_index\n",
    "\n",
    "        reward_sum = 0.0\n",
    "        \n",
    "        my_step = 0\n",
    "        new_line = dict(x=[my_step], y=[0])\n",
    "        obs = env.reset()\n",
    "        stretch_pixel = torch.zeros_like(obs).squeeze()\n",
    "        stretch_pixel[0,0] = 3\n",
    "        agent.reset()\n",
    "                \n",
    "        if agent_on:\n",
    "            action = agent(obs) \n",
    "        else:\n",
    "            action = torch.zeros_like(action)\n",
    "            \n",
    "        padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "        \n",
    "        my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        my_img[my_img > 3.0] = 3.0\n",
    "        (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        new_data = dict(my_image=[my_img])\n",
    "        \n",
    "        \n",
    "        my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "        new_weights = dict(my_image=[my_weights])\n",
    "        \n",
    "        \n",
    "        source.stream(new_data, rollover=1)\n",
    "        source_plot.stream(new_line, rollover=2)\n",
    "        \n",
    "        source_weights.stream(new_weights, rollover=1)\n",
    "        \n",
    "        message.text = f\"step {my_step} \\n\"\\\n",
    "                f\"{policy_list[0]}\"\n",
    "        \n",
    "        rewards = np.array([0])\n",
    "        \n",
    "        source_plot.stream(new_line, rollover=1)\n",
    "        source.stream(new_data, rollover=8)\n",
    "        \n",
    "        env.rules_from_string(rules[rule_index])\n",
    "        rule_index += 1\n",
    "        \n",
    "    \n",
    "    def set_birth_rules():\n",
    "        env.birth_rule_from_string(input_birth.value)\n",
    "\n",
    "        my_message = \"Rules updated to B\"\n",
    "\n",
    "        for elem in env.birth:\n",
    "            my_message += str(elem)\n",
    "        my_message += \"/S\"    \n",
    "\n",
    "        for elem in env.survive:\n",
    "            my_message += str(elem)\n",
    "\n",
    "        message.text = my_message\n",
    "\n",
    "    def set_survive_rules():\n",
    "        env.survive_rule_from_string(input_survive.value)\n",
    "\n",
    "        my_message = \"Rules updated to B\"\n",
    "\n",
    "        for elem in env.birth:\n",
    "            my_message += str(elem)\n",
    "        my_message += \"/S\"    \n",
    "\n",
    "        for elem in env.survive:\n",
    "            my_message += str(elem)\n",
    "\n",
    "        message.text = my_message\n",
    "   \n",
    "    def human_toggle(event):\n",
    "        global action\n",
    "        \n",
    "        if not(agent_on):\n",
    "            coords = [np.round(env.height*event.y/256-0.5), np.round(env.width*event.x/256-0.5)]\n",
    "            offset_x = (env.height - env.action_height) / 2\n",
    "            offset_y = (env.width - env.action_width) / 2\n",
    "\n",
    "            coords[0] = coords[0] - offset_x\n",
    "            coords[1] = coords[1] - offset_y\n",
    "\n",
    "            coords[0] = np.uint8(np.clip(coords[0], 0, env.action_height-1))\n",
    "            coords[1] = np.uint8(np.clip(coords[1], 0, env.action_height-1))\n",
    "\n",
    "            action[:, :, coords[0], coords[1]] = 1.0 * (not(action[:, :, coords[0], coords[1]]))\n",
    "\n",
    "            #padded_action = stretch_pixel/2 + env.action_padding(action).squeeze()\n",
    "            padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "\n",
    "            my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "            my_img[my_img > 3.0] = 3.0\n",
    "            (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "            new_data = dict(my_image=[my_img])\n",
    "\n",
    "            source.stream(new_data, rollover=8)\n",
    "                \n",
    "    def agent_off():\n",
    "        \n",
    "        global agent_on\n",
    "        global rule_index\n",
    "        \n",
    "        agent_on = False\n",
    "\n",
    "        rule_index = 0\n",
    "        reset_next_ruleset()\n",
    "\n",
    "        go()\n",
    "        \n",
    "        #doc = curdoc()\n",
    "        doc.add_root(control_layout)\n",
    "        \n",
    "        doc.add_root(rule_layout)\n",
    "    \n",
    "    \n",
    "    def start():\n",
    "        global my_period\n",
    "        global rule_index\n",
    "        global max_steps\n",
    "\n",
    "        rules = [\"B3/S23\", \\\n",
    "            \"B368/S245\", \\\n",
    "            \"B3678/S34678\"]\n",
    "        \n",
    "        shuffle(rules)\n",
    "        \n",
    "        doc.add_root(display_layout)\n",
    "\n",
    "\n",
    "        rule_index = 0\n",
    "        max_steps = 32\n",
    "\n",
    "        agent_done = False\n",
    "\n",
    "        \n",
    "        #doc.add_root(rule_layout)\n",
    "        doc.add_root(message_layout)\n",
    "        \n",
    "        doc.add_root(message1)\n",
    "        doc.add_root(message2)\n",
    "        doc.add_root(message3)\n",
    "        doc.add_root(message4)\n",
    "        doc.add_root(message5)\n",
    "        \n",
    "        \n",
    "        reset_next_ruleset()\n",
    "        \n",
    "        message.text = \"\"\n",
    "        message1.text = f\"Can you beat the bot?\"\n",
    "        message2.text = f\" You are tasked with maximizing the mean reward displayed in the top right by clicking cells in the {act_dim} by {act_dim} 'action space' in the center of the grid on the left.\"\n",
    "        message3.text = f\" The grid universe will cycle through {len(rules)} different shuffled rulesets, your only guide is the reward signal!\"\n",
    "        message4.text = f\" Your adversary has never trained on these rulesets either, and starts from a random initialization of weights.\"\n",
    "        message5.text = f\" Click 'Go!' to get started, HARLI will go first. ___ (I'm sorry this text is small)\"\n",
    "\n",
    "        doc.add_root(button_start)\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "    global obs\n",
    "    global my_period\n",
    "    global agent_on\n",
    "    global action\n",
    "    global reward_sum\n",
    "    global max_steps\n",
    "    global rule_index\n",
    "    global human\n",
    "    \n",
    "    dim_wh = 24\n",
    "    dim_ww = 24\n",
    "    \n",
    "    obs = env.reset()\n",
    "    my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "    \n",
    "    p = figure(plot_width=3*256, plot_height=3*256, title=\"CA Universe\")\n",
    "    p_plot = figure(plot_width=int(1.25*256), plot_height=int(1.25*256), title=\"'Reward'\")\n",
    "    p_weights = figure(plot_width=int(1.255*256), plot_height=int(1.25*256), title=\"Weights\")\n",
    "\n",
    "    \n",
    "    reward_sum = 0.0\n",
    "    \n",
    "    my_period = 128\n",
    "    \n",
    "    agent_on = True\n",
    "    action = torch.zeros(1, 1, env.action_height, env.action_width)\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(my_image=[obs.squeeze().cpu().numpy()]))\n",
    "    source_plot = ColumnDataSource(data=dict(x=np.arange(1), y=np.arange(1)*0))\n",
    "    \n",
    "    source_weights = ColumnDataSource(data=dict(my_image=[my_weights]))\n",
    "    \n",
    "    img = p.image(image='my_image',x=0, y=0, dw=256, dh=256, palette=\"Magma256\", source=source)\n",
    "    line_plot = p_plot.line(line_width=3, color=\"firebrick\", source=source_plot)\n",
    "    \n",
    "    img_w = p_weights.image(image='my_image',x=0, y=0, dw=240, dh=240, palette=\"Magma256\", source=source_weights)\n",
    "    \n",
    "    button_go = Button(sizing_mode=\"stretch_width\", label=\"Run >\")     \n",
    "    button_slower = Button(sizing_mode=\"stretch_width\",label=\"<< Slower\")\n",
    "    button_faster = Button(sizing_mode=\"stretch_width\",label=\"Faster >>\")\n",
    "    \n",
    "    button_next_ruleset = Button(sizing_mode=\"stretch_width\", label=\"Next ruleset\")\n",
    "    button_start = Button(sizing_mode=\"stretch_width\", label=\"Go!\")\n",
    "    \n",
    "\n",
    "    message = Paragraph(default_size=900)\n",
    "    \n",
    "    message1 = Paragraph(default_size=900)\n",
    "    message2 = Paragraph(default_size=900)\n",
    "    message3 = Paragraph(default_size=900)\n",
    "    message4 = Paragraph(default_size=900)\n",
    "    message5 = Paragraph(default_size=900)\n",
    "       \n",
    "    p.on_event(Tap, human_toggle)\n",
    "\n",
    "    button_go.on_click(go)\n",
    "    button_start.on_click(go)\n",
    "    button_faster.on_click(faster)\n",
    "    button_slower.on_click(slower)\n",
    "    button_next_ruleset.on_click(reset_next_ruleset)\n",
    "    \n",
    "    control_layout = row(button_slower, button_go, button_faster)\n",
    "    \n",
    "    rule_layout = row(button_next_ruleset)\n",
    "    \n",
    "    display_layout = row(p, column(p_plot, p_weights))\n",
    "    message_layout = row(message)\n",
    "\n",
    "    \n",
    "    start()\n",
    "\n",
    "    \n",
    "\n",
    "show(modify_doc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb453075",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(TextInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b44ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paragraph.properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb64084",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Can you beat the bot? \\n\"\\\n",
    "                f\"You are tasked with maximizing the mean reward displayed in the top right \\n\"\\\n",
    "                f\"by manipulating the {act_dim} by {act_dim} 'action space' in the center off the grid on the left.\"\\\n",
    "                f\"The grid universe will cycle through {len(rules)} different shuffled rulesets, your only guide is the reward signal!\"\\\n",
    "                f\"Your adversary has never trained on these rulesets either, and starts from a random initialization of weights.\\n\"\\\n",
    "                f\"Click 'Go!' to get started, HARLI will go first. (I'm sorry this text is small)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e8c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
