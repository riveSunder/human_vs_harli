{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553f16d9",
   "metadata": {},
   "source": [
    "# Can you Beat the Bot? \n",
    "\n",
    "\n",
    "\n",
    "This notebook pits you, the human (?), against a reward-hacking and meta-learning machine agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c91b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import os \n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from carle.env import CARLE\n",
    "from carle.mcl import CornerBonus, SpeedDetector, PufferDetector, AE2D, RND2D\n",
    "from game_of_carle.agents.harli import HARLI\n",
    "from game_of_carle.agents.carla import CARLA\n",
    "from game_of_carle.agents.grnn import ConvGRNN\n",
    "from game_of_carle.agents.toggle import Toggle\n",
    "\n",
    "import bokeh\n",
    "import bokeh.io as bio\n",
    "from bokeh.io import output_notebook, show, curdoc\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import TextInput, Button, Paragraph\n",
    "from bokeh.models import ColumnDataSource\n",
    "\n",
    "from bokeh.events import DoubleTap, Tap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "my_cmap = plt.get_cmap(\"viridis\")\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_string = \"cpu\"\n",
    "\n",
    "obs_dim = 32\n",
    "act_dim = 6\n",
    "\n",
    "agent =  HARLI(device=device_string, obs_dim=obs_dim, act_dim=act_dim)\n",
    "\n",
    "policy_list = []\n",
    "\n",
    "directory_list = os.listdir(\"../policies/\")\n",
    "\n",
    "for filename in directory_list:\n",
    "    \n",
    "    if \"HARLI\" in filename and \"glider\" in filename:\n",
    "        \n",
    "        policy_list.append(os.path.join(\"..\", \"policies\", filename))\n",
    "        \n",
    "policy_list.sort()\n",
    "# instantiate CARLE with a speed detection wrapper\n",
    "env = CARLE(height=obs_dim, width=obs_dim, action_height=act_dim, action_width=act_dim, device=device_string)\n",
    "env = SpeedDetector(env)\n",
    "\n",
    "# set rules\n",
    "# this agent was trained on B3/S023, B3/S236, B3/S237, and B3/S238\n",
    "my_rules = \"B3678/S34678\"\n",
    "\n",
    "agent.set_params(np.load(policy_list[0]))\n",
    "print(f\"{len(policy_list)} HARLI mobility policies found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82c549",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def modify_doc(doc):\n",
    "      \n",
    "    def agent_off():\n",
    "        \n",
    "        global agent_on\n",
    "        global rule_index\n",
    "        global my_period\n",
    "        \n",
    "        my_period = 512\n",
    "        agent_on = False\n",
    "\n",
    "        rule_index = 0\n",
    "        reset_next_ruleset()\n",
    "        \n",
    "        button_go.label == \"Run >\"\n",
    "        \n",
    "        doc.add_root(control_layout)\n",
    "        \n",
    "        p.title = \"Human's Turn\"\n",
    "        \n",
    "    def summary_screen():\n",
    "        global p_bar\n",
    "        \n",
    "        if button_go.label != \"Run >\":\n",
    "\n",
    "            doc.remove_periodic_callback(doc.session_callbacks[0])\n",
    "            button_go.label = \"Run >\"\n",
    "\n",
    "        my_width = 0.25\n",
    "        x = [0, 2, 4 ]\n",
    "        x2 = [elem+my_width*2 for elem in x]\n",
    "\n",
    "        harli_top = [elem[1] / max([100, elem[2]]) for elem in harli_scores]\n",
    "        human_top = [elem[1] / max([100, elem[2]]) for elem in human_scores]\n",
    "        \n",
    "        \n",
    "        p_bar = figure(plot_width=3*256, plot_height=3*256, title=\"BAR PLOT!\")\n",
    "\n",
    "        p_bar.vbar(x, width=0.5, top=harli_top, color=[[elem * 255 for elem in my_cmap(.35)]]*3, legend_label=\"BOT\") \n",
    "        p_bar.vbar(x2, width=0.5, top=human_top,color=[[elem * 255 for elem in my_cmap(.75)]]*3, legend_label=\"HMN                         \")\n",
    "        p_bar.vbar([6], width=0, top=0.0)\n",
    "        \n",
    "        doc.add_root(p_bar)\n",
    "        \n",
    "        doc.add_root(message1)\n",
    "        doc.add_root(message2)\n",
    "        doc.add_root(message3)\n",
    "        doc.add_root(message4)\n",
    "        doc.add_root(message5)\n",
    "        doc.add_root(message6)\n",
    "        \n",
    "        human_average = np.mean([elem[1] / max([100, elem[2]]) for elem in human_scores])\n",
    "        harli_average = np.mean([elem[1] / max([100, elem[2]]) for elem in harli_scores])\n",
    "        \n",
    "        message1.text = \"**************************************************** REWARDS per step ****************************************************\"\n",
    "        message2.text = \"__rules___________________________________________human_____________________________________HARLI__\"\n",
    "        message3.text = f\"{harli_scores[0][0]}_____________________________________{human_scores[0][1] / max([100, human_scores[0][2]]):.4f}\"\\\n",
    "                f\"_____________________________________{harli_scores[0][1] / max([100, harli_scores[0][2]]):.4f}\"\n",
    "        message4.text = f\"{harli_scores[1][0]}_____________________________________{human_scores[1][1] / max([100, human_scores[1][2]]):.4f}\"\\\n",
    "                f\"_____________________________________{harli_scores[1][1] / max([100, harli_scores[1][2]]):.4f}\"\n",
    "        message5.text = f\"{harli_scores[2][0]}_____________________________________{human_scores[2][1] / max([100, human_scores[2][2]]):.4f}\"\\\n",
    "                f\"_____________________________________{harli_scores[2][1] / max([100, harli_scores[2][2]]):.4f}\"\n",
    "        message6.text = f\"Average__________________________________________{human_average:.4f}_____________________________________{harli_average:.4f}\"\n",
    "    \n",
    "        doc.remove_root(display_layout)\n",
    "        doc.remove_root(button_go)\n",
    "        doc.remove_root(control_layout)\n",
    "        doc.remove_root(rule_layout)\n",
    "        \n",
    "        doc.remove_root(message_layout)\n",
    "        \n",
    "        doc.add_root(button_start_over)\n",
    "            \n",
    "    def update():\n",
    "        global obs\n",
    "        global stretch_pixel\n",
    "        global action\n",
    "        global agent_on\n",
    "        global my_step\n",
    "        global max_steps\n",
    "        global rule_index\n",
    "        global rewards\n",
    "        global reward_sum\n",
    "               \n",
    "            \n",
    "        if agent_on and rule_index == len(rules) and my_step >= max_steps:\n",
    "            \n",
    "            harli_scores.append((rules[rule_index-1], reward_sum, my_step))\n",
    "            agent_off()\n",
    "            \n",
    "        elif not(agent_on) and rule_index == len(rules) and my_step >= max_steps:\n",
    "            \n",
    "            human_scores.append((rules[rule_index-1], reward_sum, my_step))\n",
    "            summary_screen()\n",
    "        \n",
    "        elif agent_on and my_step >= max_steps:\n",
    "            \n",
    "            harli_scores.append((rules[rule_index-1], reward_sum, my_step))\n",
    "            reset_next_ruleset()\n",
    "            \n",
    "        elif not(agent_on) and my_step >= max_steps:\n",
    "            \n",
    "            human_scores.append((rules[rule_index-1], reward_sum, my_step))\n",
    "            reset_next_ruleset()\n",
    "            \n",
    "        else:\n",
    "\n",
    "            obs, r, d, i = env.step(action)\n",
    "            rewards = np.append(rewards, r.cpu().numpy().item())\n",
    "            if agent_on:\n",
    "                action = agent(obs) \n",
    "            else:\n",
    "                action = torch.zeros_like(action)\n",
    "\n",
    "            padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "\n",
    "            my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "            my_img[my_img > 3.0] = 3.0\n",
    "            (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "\n",
    "            new_data = dict(my_image=[my_img])\n",
    "\n",
    "            my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "            new_weights = dict(my_image=[my_weights])\n",
    "\n",
    "            #new_line = dict(x=np.arange(my_step+2), y=rewards)\n",
    "            new_line = dict(x=[my_step], y=[r.cpu().numpy().item()])\n",
    "\n",
    "            source.stream(new_data, rollover=1)\n",
    "            source_plot.stream(new_line, rollover=2000)\n",
    "\n",
    "            source_weights.stream(new_weights, rollover=1)\n",
    "\n",
    "            my_step += 1\n",
    "            reward_sum += r.item()\n",
    "            turn_msg = \"Bot's turn: \" if agent_on  else \"Human's turn: \"\n",
    "            message.text = f\"{turn_msg}step {my_step}, reward: {r.item():.4f}, mean reward per step: {(reward_sum/my_step):.4f} \\n\"\\\n",
    "                    f\" rule index = {rule_index}\"\n",
    "\n",
    "    def go():\n",
    "       \n",
    "        if button_go.label == \"Run >\":\n",
    "            my_callback = doc.add_periodic_callback(update, my_period)\n",
    "            button_go.label = \"Pause\"\n",
    "            #doc.remove_periodic_callback(my_callback)\n",
    "            \n",
    "        else:\n",
    "            doc.remove_periodic_callback(doc.session_callbacks[0])\n",
    "            button_go.label = \"Run >\"\n",
    "            \n",
    "        doc.remove_root(button_start)\n",
    "        \n",
    "        doc.remove_root(message1)\n",
    "        doc.remove_root(message2)\n",
    "        doc.remove_root(message3)\n",
    "        doc.remove_root(message4)\n",
    "        doc.remove_root(message5)\n",
    "        \n",
    "        \n",
    "        doc.add_root(control_layout)\n",
    "    \n",
    "    def reset_next_ruleset():\n",
    "       \n",
    "        global obs\n",
    "        global action\n",
    "        global stretch_pixel\n",
    "        global my_step\n",
    "        global rewards\n",
    "        global use_spaceship\n",
    "        global rewards\n",
    "        global reward_sum\n",
    "        global rule_index\n",
    "        global human_scores\n",
    "        global harli_scores\n",
    "\n",
    "\n",
    "        reward_sum = 0.0\n",
    "        \n",
    "        my_step = 0\n",
    "        new_line = dict(x=[my_step], y=[0])\n",
    "        obs = env.reset()\n",
    "        stretch_pixel = torch.zeros_like(obs).squeeze()\n",
    "        stretch_pixel[0,0] = 3\n",
    "        agent.reset()\n",
    "                \n",
    "        if agent_on:\n",
    "            action = agent(obs) \n",
    "        else:\n",
    "            action = torch.zeros_like(action)\n",
    "            \n",
    "        padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "        \n",
    "        my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        my_img[my_img > 3.0] = 3.0\n",
    "        (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "        new_data = dict(my_image=[my_img])\n",
    "        \n",
    "        \n",
    "        my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "        new_weights = dict(my_image=[my_weights])\n",
    "        \n",
    "        \n",
    "        source.stream(new_data, rollover=1)\n",
    "        source_plot.stream(new_line, rollover=2)\n",
    "        \n",
    "        source_weights.stream(new_weights, rollover=1)\n",
    "        \n",
    "        message.text = f\"step {my_step} \\n\"\\\n",
    "                f\"{policy_list[0]}\"\n",
    "        \n",
    "        rewards = np.array([0])\n",
    "        \n",
    "        source_plot.stream(new_line, rollover=1)\n",
    "        source.stream(new_data, rollover=8)\n",
    "        \n",
    "        env.rules_from_string(rules[rule_index])\n",
    "        rule_index += 1\n",
    "        \n",
    "        if (button_go.label == \"Pause\") and not(agent_on) and rule_index != 0:\n",
    "            go()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def start():\n",
    "        global my_period\n",
    "        global my_step\n",
    "        global rule_index\n",
    "        global max_steps\n",
    "        global human_scores\n",
    "        global harli_scores\n",
    "        global rules\n",
    "        global p_bar\n",
    "        global agent_on\n",
    "        \n",
    "        rule_index = 0\n",
    "        max_steps = 128\n",
    "\n",
    "        agent_on = True\n",
    "        \n",
    "        harli_scores = []\n",
    "        human_scores = []\n",
    "\n",
    "        rules = [\"B3___/S23___\", \\\n",
    "            \"B368_/S245__\", \\\n",
    "            \"B3678/S34678\"]\n",
    "        \n",
    "        shuffle(rules)\n",
    "        \n",
    "        p_bar = figure(plot_width=3*256, plot_height=3*256, title=\"BAR PLOT!\")\n",
    "        \n",
    "        doc.remove_root(message1)\n",
    "        doc.remove_root(message2)\n",
    "        doc.remove_root(message3)\n",
    "        doc.remove_root(message4)\n",
    "        doc.remove_root(message5)\n",
    "        doc.remove_root(message6)\n",
    "        doc.remove_root(p_bar)\n",
    "        \n",
    "        doc.add_root(display_layout)\n",
    "        doc.remove_root(button_start_over)\n",
    "        \n",
    "        #doc.add_root(rule_layout)\n",
    "        doc.add_root(message_layout)\n",
    "        \n",
    "        doc.add_root(message1)\n",
    "        doc.add_root(message2)\n",
    "        doc.add_root(message3)\n",
    "        doc.add_root(message4)\n",
    "        doc.add_root(message5)\n",
    "        \n",
    "        reset_next_ruleset()\n",
    "        \n",
    "        message.text = \"\"\n",
    "        message1.text = f\"Can you beat the bot?\"\n",
    "        message2.text = f\" You are tasked with maximizing the mean reward displayed in the top right by clicking cells in the {act_dim} by {act_dim} 'action space' in the center of the grid on the left.\"\n",
    "        message3.text = f\" The grid universe will cycle through {len(rules)} different shuffled rulesets, your only guide is the reward signal!\"\n",
    "        message4.text = f\" Your adversary has never trained on these rulesets either, and starts from a random initialization of weights.\"\n",
    "        message5.text = f\" Click 'Go!' to get started, HARLI will go first. ___ (I'm sorry this text is small)\"\n",
    "\n",
    "        doc.add_root(button_start)\n",
    "        \n",
    "        p.title = \"Bot's Turn\"\n",
    "        \n",
    "\n",
    "    def faster():\n",
    "        \n",
    "        \n",
    "        global my_period\n",
    "        my_period = max([my_period * 0.5, 32])\n",
    "        go()\n",
    "        go()\n",
    "        \n",
    "    def slower():\n",
    "        \n",
    "        global my_period\n",
    "        my_period = min([my_period * 2, 8192])\n",
    "        go()\n",
    "        go()\n",
    "        \n",
    "    def human_toggle(event):\n",
    "        global action\n",
    "        \n",
    "        if not(agent_on):\n",
    "            coords = [np.round(env.height*event.y/256-0.5), np.round(env.width*event.x/256-0.5)]\n",
    "            offset_x = (env.height - env.action_height) / 2\n",
    "            offset_y = (env.width - env.action_width) / 2\n",
    "\n",
    "            coords[0] = coords[0] - offset_x\n",
    "            coords[1] = coords[1] - offset_y\n",
    "\n",
    "            coords[0] = np.uint8(np.clip(coords[0], 0, env.action_height-1))\n",
    "            coords[1] = np.uint8(np.clip(coords[1], 0, env.action_height-1))\n",
    "\n",
    "            action[:, :, coords[0], coords[1]] = 1.0 * (not(action[:, :, coords[0], coords[1]]))\n",
    "\n",
    "            #padded_action = stretch_pixel/2 + env.action_padding(action).squeeze()\n",
    "            padded_action = stretch_pixel/2 + env.inner_env.action_padding(action).squeeze()\n",
    "\n",
    "            my_img = (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "            my_img[my_img > 3.0] = 3.0\n",
    "            (padded_action*2 + obs.squeeze()).cpu().numpy()\n",
    "            new_data = dict(my_image=[my_img])\n",
    "\n",
    "            source.stream(new_data, rollover=8)\n",
    "                \n",
    "        \n",
    "    global obs\n",
    "    global my_period\n",
    "    global agent_on\n",
    "    global action\n",
    "    global reward_sum\n",
    "    global max_steps\n",
    "    global rule_index\n",
    "    global human_scores\n",
    "    global harli_scores\n",
    " \n",
    "    dim_wh = 24\n",
    "    dim_ww = 24\n",
    "    \n",
    "    obs = env.reset()\n",
    "    my_weights = agent.get_weights().reshape(dim_wh, dim_ww)\n",
    "    \n",
    "    p = figure(plot_width=3*256, plot_height=3*256, title=\"CA Universe\")\n",
    "    p_plot = figure(plot_width=int(1.25*256), plot_height=int(1.25*256), title=\"'Reward'\")\n",
    "    p_weights = figure(plot_width=int(1.255*256), plot_height=int(1.25*256), title=\"Weights\")\n",
    "    \n",
    "    p_bar = figure(plot_width=3*256, plot_height=3*256, title=\"BAR PLOT!\")\n",
    "    reward_sum = 0.0\n",
    "    \n",
    "    my_period = 32\n",
    "    \n",
    "    agent_on = True\n",
    "    action = torch.zeros(1, 1, env.action_height, env.action_width)\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(my_image=[obs.squeeze().cpu().numpy()]))\n",
    "    source_plot = ColumnDataSource(data=dict(x=np.arange(1), y=np.arange(1)*0))\n",
    "    \n",
    "    source_weights = ColumnDataSource(data=dict(my_image=[my_weights]))\n",
    "    \n",
    "    img = p.image(image='my_image',x=0, y=0, dw=256, dh=256, palette=\"Magma256\", source=source)\n",
    "    line_plot = p_plot.line(line_width=3, color=\"firebrick\", source=source_plot)\n",
    "    \n",
    "    img_w = p_weights.image(image='my_image',x=0, y=0, dw=240, dh=240, palette=\"Magma256\", source=source_weights)\n",
    "    \n",
    "    button_go = Button(sizing_mode=\"stretch_width\", label=\"Run >\")     \n",
    "    button_slower = Button(sizing_mode=\"stretch_width\",label=\"<< Slower\")\n",
    "    button_faster = Button(sizing_mode=\"stretch_width\",label=\"Faster >>\")\n",
    "    \n",
    "    button_next_ruleset = Button(sizing_mode=\"stretch_width\", label=\"Next ruleset\")\n",
    "    button_start = Button(sizing_mode=\"stretch_width\", label=\"Go!\")\n",
    "    \n",
    "    button_start_over = Button(sizing_mode=\"stretch_width\", label=\"Go again!\")\n",
    "    \n",
    "\n",
    "    message = Paragraph(default_size=900)\n",
    "    \n",
    "    message1 = Paragraph(default_size=900)\n",
    "    message2 = Paragraph(default_size=900)\n",
    "    message3 = Paragraph(default_size=900)\n",
    "    message4 = Paragraph(default_size=900)\n",
    "    message5 = Paragraph(default_size=900)\n",
    "    \n",
    "    message6 = Paragraph(default_size=900)\n",
    "       \n",
    "    p.on_event(Tap, human_toggle)\n",
    "\n",
    "    button_start_over.on_click(start)\n",
    "    button_go.on_click(go)\n",
    "    button_start.on_click(go)\n",
    "    button_faster.on_click(faster)\n",
    "    button_slower.on_click(slower)\n",
    "    button_next_ruleset.on_click(reset_next_ruleset)\n",
    "    \n",
    "    control_layout = row(button_slower, button_go, button_faster)\n",
    "    \n",
    "    rule_layout = row(button_next_ruleset)\n",
    "    \n",
    "    display_layout = row(p, column(p_plot, p_weights))\n",
    "    message_layout = row(message)\n",
    "\n",
    "    \n",
    "    start()\n",
    "\n",
    "    \n",
    "\n",
    "show(modify_doc)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
